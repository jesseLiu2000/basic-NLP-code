{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Lu59zrwJF9x8"],"mount_file_id":"1LLj3SfmV8NGTlmfs2Qh_S3Y4LNnpHMk4","authorship_tag":"ABX9TyNiAayZ+ulkdj/wlpzHoeSE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Dataset and DataLoader"],"metadata":{"id":"Lu59zrwJF9x8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DXRkmOlriFC","executionInfo":{"status":"ok","timestamp":1667019517044,"user_tz":300,"elapsed":152,"user":{"displayName":"Jesse Liu","userId":"09638298407322875956"}},"outputId":"e5aaac4e-aa40-4d76-9be3-13b18cc3d905"},"outputs":[{"output_type":"stream","name":"stdout","text":["[3, 1]\n","[7, 5]\n","[6, 4]\n","[2]\n","[2, 1]\n","[3, 4]\n","[7, 6]\n","[5]\n"]}],"source":["import random\n","list1 = [1, 2, 3, 4, 5, 6, 7]\n","\n","batch_size = 2\n","epoch = 2\n","shuffle = True\n","\n","\n","for ep in range(epoch):\n","  if shuffle:\n","    random.shuffle(list1)\n","  for i in range(0, len(list1), batch_size):\n","    batch_data = list1[i:i+batch_size]\n","    print(batch_data)\n"]},{"cell_type":"code","source":["import random\n","import numpy as np\n","\n","\n","class MyDataset:\n","    def __init__(self, all_data, batch_size, shuffle):\n","        self.all_data = all_data\n","        self.bs = batch_size\n","        self.shuffle = shuffle\n","        self.cursor = 0\n","\n","    def __iter__(self):\n","        return DataLoader(self)\n","        # pass\n","        # if self.shuffle:\n","        #     random.shuffle(self.all_data)\n","        #     self.cursor = 0\n","        # return self\n","    \n","    def __len__(self):\n","        return len(self.all_data)\n","\n","    # def __next__(self):\n","    #     if self.cursor >= len(self.all_data):\n","    #         raise StopIteration\n","    # \n","    #     batch_data = self.all_data[self.cursor: self.cursor+self.bs]\n","    #     self.cursor += self.bs\n","    #     return batch_data\n","\n","\n","class DataLoader():\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","        self.index = [i for i in range(len(self.dataset))]\n","        if self.shuffle:\n","            random.shuffle(self.index)\n","        self.cursor = 0\n","    \n","    def __next__(self):\n","        if self.cursor >= len(self.dataset.all_data):\n","            raise StopIteration\n","        \n","        # shuffle the index and then pick them instead of shuffling the data\n","        index = self.dataset.all_data[self.cursor: self.cursor + self.dataset.bs]\n","        batch_data = self.dataset.all_data[index]\n","        self.cursor += self.dataset.bs\n","        return batch_data\n","\n","\n","if __name__ == '__main__':\n","\n","    all_data = np.array([1, 2, 3, 4, 5, 6, 7])\n","    batch_size = 2\n","    shuffle = True\n","    epoch = 2\n","\n","    dataset = MyDataset(all_data, batch_size, shuffle)\n","\n","    for ep in range(epoch):\n","        for data in dataset:\n","            print(data)\n","\n","\n"],"metadata":{"id":"ec8_UX2OGBBm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logistics Regression"],"metadata":{"id":"eU3ncbDZXfQB"}},{"cell_type":"code","source":["import numpy as np\n","\n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))\n","\n","if __name__ == \"__main__\":\n","    count = 0\n","    dogs = np.array([[8.9,12],[9,11],[10,13],[9.9,11.2],[12.2,10.1],[9.8,13],[8.8,11.2]],dtype = np.float32)   # 0\n","    cats = np.array([[3,4],[5,6],[3.5,5.5],[4.5,5.1],[3.4,4.1],[4.1,5.2],[4.4,4.4]],dtype = np.float32)        # 1\n","\n","    labels = np.array([0]*7 + [1]* 7,dtype = np.int32).reshape(-1,1)\n","\n","    X = np.vstack((dogs,cats))\n","\n","    k = np.random.normal(0,1,size=(2,1))\n","    b = 0\n","    epoch = 1000\n","    lr = 0.05\n","\n","    for e in range(epoch):\n","        p = X @ k + b\n","        pre = sigmoid(p)\n","\n","        loss = -np.sum(labels * np.log(pre) + (1-labels) * np.log(1-pre))\n","\n","        G = pre - labels\n","        delta_k = X.T @ G\n","        delta_b = np.sum(G)\n","\n","        k = k - lr * delta_k\n","        b = b - lr * delta_b\n","        # print(loss)\n","\n","    while count < 2:\n","        count += 1\n","        f1 = float(input('Please input the length of hair:'))\n","        f2 = float(input(\"Please input the length of legs:\"))\n","\n","        test_x = np.array([f1,f2]).reshape(1,2)\n","        p = sigmoid(test_x @ k + b )\n","        if p >0.5:\n","            print(\"CAT\")\n","        else:\n","            print(\"DOG\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQLAJsgmXjm6","executionInfo":{"status":"ok","timestamp":1667025208479,"user_tz":300,"elapsed":17242,"user":{"displayName":"Jesse Liu","userId":"09638298407322875956"}},"outputId":"aeb100df-6b5c-4dcc-f28f-078e90e797fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Please input the length of hair:5\n","Please input the length of legs:8\n","CAT\n","Please input the length of hair:9\n","Please input the length of legs:3\n","CAT\n"]}]},{"cell_type":"markdown","source":["## BP"],"metadata":{"id":"O0Cm6zGecjG3"}},{"cell_type":"code","source":["import numpy as np\n","import struct\n","import matplotlib.pyplot as plt\n","\n","def load_labels(file):  \n","    with open(file, \"rb\") as f:\n","        data = f.read()\n","    return np.asanyarray(bytearray(data[8:]), dtype=np.int32)\n","\n","\n","def load_images(file):  \n","    with open(file, \"rb\") as f:\n","        data = f.read()\n","    magic_number, num_items, rows, cols = struct.unpack(\">iiii\", data[:16])\n","    return np.asanyarray(bytearray(data[16:]), dtype=np.uint8).reshape(num_items, -1)\n","\n","\n","def make_one_hot(labels,class_num=10):\n","    result = np.zeros((len(labels),class_num))\n","    for index,lab in enumerate(labels):\n","        result[index][lab] = 1\n","    return result\n","\n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))\n","\n","def softmax(x):\n","    ex = np.exp(x)\n","    sum_ex = np.sum(ex,axis=1,keepdims=True)\n","    return  ex/sum_ex\n","\n","\n","\n","if __name__ == \"__main__\":\n","    train_datas = load_images(\"/content/drive/MyDrive/N_F/data/train-images.idx3-ubyte\") / 255\n","    train_label = make_one_hot(load_labels(\"/content/drive/MyDrive/N_F/data/train-labels.idx1-ubyte\"))\n","\n","    test_datas = load_images(\"/content/drive/MyDrive/N_F/data/t10k-images.idx3-ubyte\") / 255\n","    test_label = load_labels(\"/content/drive/MyDrive/N_F/data/t10k-labels.idx1-ubyte\")\n","\n","    epoch = 20\n","    batch_size = 200\n","    lr = 0.01\n","\n","    hidden_num = 256\n","    w1 = np.random.normal(0,1,size=(784,hidden_num))\n","    w2 = np.random.normal(0,1,size=(hidden_num,10))\n","\n","\n","    batch_times = int(np.ceil(len(train_datas) / batch_size))\n","    for e in range(epoch):\n","        for batch_index in range(batch_times):\n","    \n","            batch_x = train_datas[batch_index*batch_size:(batch_index+1)*batch_size]\n","            batch_label = train_label[batch_index*batch_size:(batch_index+1)*batch_size]\n","\n","           \n","            h = batch_x @ w1\n","            sig_h = sigmoid(h)\n","            p = sig_h @ w2\n","            pre = softmax(p)\n","\n","      \n","            loss = -np.sum(batch_label * np.log(pre))/batch_size\n","\n","\n","            G2 = (pre - batch_label)/batch_size\n","            delta_w2 = sig_h.T @ G2\n","            delta_sig_h = G2 @ w2.T\n","            delta_h = delta_sig_h * sig_h * (1-sig_h)\n","            delta_w1 = batch_x.T @ delta_h\n","\n","\n","            w1 = w1 - lr * delta_w1\n","            w2 = w2 - lr * delta_w2\n","\n","        h = test_datas @ w1\n","        sig_h = sigmoid(h)\n","        p = sig_h @ w2\n","        pre = softmax(p)\n","        pre = np.argmax(pre,axis=1)\n","        acc = np.sum(pre == test_label)/10000\n","        print(acc)"],"metadata":{"id":"Wn_JtEeWclXZ"},"execution_count":null,"outputs":[]}]}