{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNn9gQ9gac2lrEJAqSlOnnx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tV02KF0r8tgc"},"outputs":[],"source":["import numpy as np\n","import struct\n","import matplotlib.pyplot as plt\n","\n","def load_labels(file):  \n","    with open(file, \"rb\") as f:\n","        data = f.read()\n","    return np.asanyarray(bytearray(data[8:]), dtype=np.int32)\n","\n","\n","def load_images(file):  \n","    with open(file, \"rb\") as f:\n","        data = f.read()\n","    magic_number, num_items, rows, cols = struct.unpack(\">iiii\", data[:16])\n","    return np.asanyarray(bytearray(data[16:]), dtype=np.uint8).reshape(num_items, -1)\n","\n","\n","def make_one_hot(labels,class_num=10):\n","    result = np.zeros((len(labels),class_num))\n","    for index,lab in enumerate(labels):\n","        result[index][lab] = 1\n","    return result\n","\n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))\n","\n","def softmax(x):\n","    ex = np.exp(x)\n","    sum_ex = np.sum(ex,axis=1,keepdims=True)\n","    return  ex/sum_ex\n","\n","\n","\n","if __name__ == \"__main__\":\n","    train_datas = load_images(\"/content/drive/MyDrive/N_F/data/train-images.idx3-ubyte\") / 255\n","    train_label = make_one_hot(load_labels(\"/content/drive/MyDrive/N_F/data/train-labels.idx1-ubyte\"))\n","\n","    test_datas = load_images(\"/content/drive/MyDrive/N_F/data/t10k-images.idx3-ubyte\") / 255\n","    test_label = load_labels(\"/content/drive/MyDrive/N_F/data/t10k-labels.idx1-ubyte\")\n","\n","    epoch = 20\n","    batch_size = 200\n","    lr = 0.01\n","\n","    hidden_num = 256\n","    w1 = np.random.normal(0,1,size=(784,hidden_num))\n","    w2 = np.random.normal(0,1,size=(hidden_num,10))\n","\n","\n","    batch_times = int(np.ceil(len(train_datas) / batch_size))\n","    for e in range(epoch):\n","        for batch_index in range(batch_times):\n","    \n","            batch_x = train_datas[batch_index*batch_size:(batch_index+1)*batch_size]\n","            batch_label = train_label[batch_index*batch_size:(batch_index+1)*batch_size]\n","\n","           \n","            h = batch_x @ w1\n","            sig_h = sigmoid(h)\n","            p = sig_h @ w2\n","            pre = softmax(p)\n","\n","      \n","            loss = -np.sum(batch_label * np.log(pre))/batch_size\n","\n","\n","            G2 = (pre - batch_label)/batch_size\n","            delta_w2 = sig_h.T @ G2\n","            delta_sig_h = G2 @ w2.T\n","            delta_h = delta_sig_h * sig_h * (1-sig_h)\n","            delta_w1 = batch_x.T @ delta_h\n","\n","\n","            w1 = w1 - lr * delta_w1\n","            w2 = w2 - lr * delta_w2\n","\n","        h = test_datas @ w1\n","        sig_h = sigmoid(h)\n","        p = sig_h @ w2\n","        pre = softmax(p)\n","        pre = np.argmax(pre,axis=1)\n","        acc = np.sum(pre == test_label)/10000\n","        print(acc)"]}]}