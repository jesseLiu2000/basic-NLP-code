{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEpbc4RCOSvVXSu4Bw/HpN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7qikKGB5sze","executionInfo":{"status":"ok","timestamp":1677753599780,"user_tz":360,"elapsed":22858,"user":{"displayName":"Jesse Liu","userId":"09638298407322875956"}},"outputId":"c3943694-702c-4eb8-b9da-0337843ac9cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1000 cost = 0.000067\n","Epoch: 2000 cost = 0.000370\n","Epoch: 3000 cost = 0.000002\n","I hate you is Bad\n"]}],"source":["# jesseLiu2000\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","\n","class TextCNN(nn.Module):\n","    def __init__(self, dropout=0.4):\n","        super(TextCNN, self).__init__()\n","        self.embedding = nn.Embedding(n_seq, embedding_size)\n","        self.cov1 = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv1d(in_channels=embedding_size, out_channels=hidden_size, kernel_size=k),\n","                nn.LeakyReLU(),\n","                nn.MaxPool1d(kernel_size=k)\n","            ) for k in kernel_size\n","        ])\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc1 = nn.Linear(hidden_size*len(kernel_size), n_class)\n","\n","    def forward(self, X):\n","        embeded_X = self.embedding(X) # [batch_size, seq_length, embedding_size]\n","        embeded_X = embeded_X.permute(0, 2, 1)\n","        coved_X = [conv(embeded_X) for conv in self.cov1]\n","        coved_X = torch.cat(coved_X, dim=1) # [batch_size, hidden_size*len(kernel_size), 1]\n","        reshaped_X = torch.reshape(coved_X, [-1, hidden_size*len(kernel_size)]) # [batch_size, hidden_size*len(kernel_size)]\n","        dropout_X = self.dropout(reshaped_X)\n","        output = self.fc1(dropout_X) # [batch_size, num_classes]\n","\n","        return output\n","\n","\n","if __name__ == '__main__':\n","    n_seq = 3\n","    embedding_size = 8\n","    hidden_size = 12\n","    kernel_size = [2, 2, 2]\n","    n_class = 2\n","\n","    sentences = ['I hate milk', \"I hate you\", \"sorry for that\", \"this is awful\", 'I like cat', \"I love you\"]\n","    labels = [0, 0, 0, 0, 1, 1]\n","\n","    word_vocab = ' '.join(sentences).split()\n","    word_vocab = list(set(word_vocab))\n","    word2index = {w: i for i, w in enumerate(word_vocab)}\n","    index2word = {i: w for i, w in enumerate(word_vocab)}\n","\n","    n_seq = len(word_vocab)\n","\n","    # inputs[batch_size, seq_length] => [6, 3]\n","    inputs = torch.LongTensor(np.asarray([np.asarray([word2index[word] for word in sent.split()]) for sent in sentences]))\n","    targets = torch.LongTensor(labels)\n","\n","    model = TextCNN()\n","\n","    lr = 1e-3\n","    epoch = 3000\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    # Train\n","    for ep in range(epoch):\n","        optimizer.zero_grad()\n","\n","        output = model(inputs)\n","        loss = criterion(output, targets)\n","        # print(output.size())\n","\n","        if (ep + 1) % 1000 == 0:\n","            print('Epoch:', '%04d' % (ep + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Test\n","    test_text = 'I hate you'\n","    tests = torch.LongTensor(np.asarray([np.asarray([word2index[n] for n in test_text.split()])]))\n","\n","    # Predict\n","    predict = model(tests).data.max(1, keepdim=True)[1]\n","    if predict[0][0] == 0:\n","        print(test_text, \"is Bad\")\n","    else:\n","        print(test_text, \"is Good\")\n","\n"]}]}