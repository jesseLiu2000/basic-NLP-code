{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJ/n54iQTKz2fHzlzIQ9yE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcDfnHc-5nQO","executionInfo":{"status":"ok","timestamp":1677753536302,"user_tz":360,"elapsed":16507,"user":{"displayName":"Jesse Liu","userId":"09638298407322875956"}},"outputId":"016427ad-0bdd-4a77-82c3-214ce046a734"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1000 cost = 0.622854\n","Epoch: 2000 cost = 0.420115\n","Epoch: 3000 cost = 0.088562\n","Epoch: 4000 cost = 0.027916\n","Epoch: 5000 cost = 0.013745\n","Input: [['I', 'hate'], ['Its', 'cold'], ['I', 'like']]\n","OutPut: ['milk', 'today', 'cat']\n"]}],"source":["# jesseliu2000\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","class NNLM(nn.Module):\n","    def __init__(self):\n","        super(NNLM, self).__init__()\n","        self.embedding = nn.Embedding(n_words, embedding_size)\n","        self.fc1 = nn.Linear(n_token * embedding_size, n_hidden1)\n","        self.fc2 = nn.Linear(n_hidden1, n_hidden2)\n","        self.fc3 = nn.Linear(n_hidden2, n_words)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, X):\n","        X1 = self.embedding(X) # [batch_size, seq_length, embedding_size]\n","        X1 = X1.view(-1, n_token * embedding_size) # [batch_size, seq_length * embedding_size]\n","        X2 = self.fc1(X1) # [batch_size, hidden_size]\n","        X_act = self.tanh(X2) # [batch_size, hidden_size]\n","        X2 = self.tanh(self.fc2(X_act))\n","        output = self.fc3(X2)\n","        # print(output.size())\n","        return output\n","\n","\n","\n","if __name__ == '__main__':\n","    n_token = 2  # number of token in one sentence\n","    n_hidden1 = 4  # number of hidden size\n","    n_hidden2 = 2  # number of hidden size\n","    embedding_size = 2\n","\n","    file_list = ['I hate milk', 'Its cold today', 'I like cat']\n","\n","    word_vocab = ' '.join(file_list).split()\n","    word_vocab = list(set(word_vocab))\n","    word2index = {w: i for i, w in enumerate(word_vocab)}\n","    index2word = {i: w for i, w in enumerate(word_vocab)}\n","\n","    n_words = len(word2index)  # number of whole word\n","\n","    input = [i.split(' ')[:-1] for i in file_list]\n","    map_input = [[word2index[i] for i in j] for j in input]\n","    target = [i.split(' ')[-1] for i in file_list]\n","    map_target = [word2index[i] for i in target]\n","\n","\n","    # promise the same input length\n","    map_input = torch.LongTensor(map_input)\n","    map_target = torch.LongTensor(map_target)\n","\n","    model = NNLM()\n","\n","    lr = 1e-3\n","    epoch = 5000\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    # Training\n","    for ep in range(epoch):\n","        optimizer.zero_grad()\n","        output = model(map_input)\n","        loss = criterion(output, map_target)\n","\n","        if (ep + 1) % 1000 == 0:\n","            print('Epoch:', '%04d' % (ep + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Test\n","    predict = model(map_input).data.max(1, keepdim=True)[1]\n","    predict = predict.squeeze()\n","    print(\"Input:\", [sentence.split()[:2] for sentence in file_list])\n","    print(\"OutPut:\", [index2word[n.item()] for n in predict])"]}]}