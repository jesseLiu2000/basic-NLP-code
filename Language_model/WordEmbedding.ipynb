{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1SRQ-MR_cHvZeX5NToFI4-Rh0LHm8bRGq","authorship_tag":"ABX9TyPajU8/1XPY4+rzuBMLO+4u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-pPVvx2izfsR","executionInfo":{"status":"ok","timestamp":1677753389936,"user_tz":360,"elapsed":316,"user":{"displayName":"Jesse Liu","userId":"09638298407322875956"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import pickle\n","import jieba\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","source":["\n","\n","def load_stop_words(file='/content/drive/MyDrive/N_F/Language_model/stopwords.txt'):\n","    with open(file, encoding='utf-8') as f:\n","        return f.read().split('\\n')\n","\n","\n","def cut_words(file='/content/drive/MyDrive/N_F/Language_model/Original_dataset.csv'):\n","    stop_word = load_stop_words()\n","\n","    result = []\n","    all_data = pd.read_csv(file, encoding='gbk', names=[\"data\"])[\"data\"]\n","    for words in all_data:\n","        c_words = jieba.lcut(words)\n","        result.append([word for word in c_words if word not in stop_word])\n","\n","    return result\n","\n","\n","def get_dict(data):\n","    index2word = []\n","    for words in data:\n","        for word in words:\n","            if word not in index2word:\n","                index2word.append(word)\n","\n","    word2index = {word: index for index, word in enumerate(index2word)}\n","    word_size = len(word2index)\n","\n","    word2onehot = {}\n","    for word, index in word2index.items():\n","        one_hot = np.zeros((1, word_size))\n","        one_hot[0, index] = 1\n","        word2onehot[word] = one_hot\n","\n","    return word2index, index2word, word2onehot\n","\n","def softmax(x):\n","    ex = np.exp(x)\n","    return ex/np.sum(ex, axis=1, keepdims = True)\n","\n","\n","if __name__ == '__main__':\n","    data = cut_words()\n","    word2index, index2word, word2onehot = get_dict(data)\n","\n","    word_size = len(word2index)\n","    embedding_num = 107\n","    lr = 0.1\n","    epoch = 10\n","    n_gram = 3\n","\n","    w1 = np.random.normal(-1, 1, size=(word_size, embedding_num))\n","    w2 = np.random.normal(-1, 1, size=(embedding_num, word_size))\n","\n","    for ep in range(epoch):\n","        for words in tqdm(data):\n","            for n_index, now_word in enumerate(words):\n","                now_word_onehot = word2onehot[now_word]\n","                other_words = words[max(n_index - n_gram, 0):n_index] + words[n_index + 1: n_index+1+n_gram]\n","                for other_word in other_words:\n","                    other_word_onehot = word2onehot[other_word]\n","\n","                    hidden = now_word_onehot @ w1\n","                    p = hidden @ w2\n","                    pre = softmax(p)\n","\n","\n","                    G2 = pre - other_word_onehot\n","                    delta_w2 = hidden.T @ G2\n","                    G1 = G2 @ w2.T\n","                    delta_w1 = now_word_onehot.T @ G1\n","\n","                    w1 -= lr * delta_w1\n","                    w2 -= lr * delta_w2\n","\n","    with open(\"word2vec.pkl\", \"wb\") as f:\n","        pickle.dump([w1, word_2_index, index_2_word, w2], f)  \n","\n"],"metadata":{"id":"gl6OKS4N79rO","executionInfo":{"status":"ok","timestamp":1677754076717,"user_tz":360,"elapsed":137,"user":{"displayName":"Jesse Liu","userId":"09638298407322875956"}}},"execution_count":3,"outputs":[]}]}