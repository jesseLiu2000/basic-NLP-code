{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPaj8sf9wTV/r6jzXfESaUO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJWf7IKi595K","executionInfo":{"status":"ok","timestamp":1677753632819,"user_tz":360,"elapsed":30868,"user":{"displayName":"Jesse Liu","userId":"09638298407322875956"}},"outputId":"bab8a7e5-4a06-4dfc-e0b6-54fcc2fef68f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1000 cost = 0.289867\n","Epoch: 2000 cost = 0.030923\n","Epoch: 3000 cost = 0.013133\n","Epoch: 4000 cost = 0.006201\n","Epoch: 5000 cost = 0.003976\n","Input: [['I', 'hate'], ['Its', 'cold'], ['I', 'like']]\n","Output: ['milk', 'today', 'cat']\n"]}],"source":["# jesseLiu2000\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","class TextRNN(nn.Module):\n","    def __init__(self):\n","        super(TextRNN, self).__init__()\n","        self.embedding = nn.Embedding(n_seq, embedding_size)\n","        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=n_hidden, num_layers=4, dropout=0.1)\n","        self.fc1 = nn.Linear(n_hidden, n_seq)\n","\n","    def forward(self, state, X):\n","        X1 = self.embedding(X)\n","        X1 = X1.float()\n","        X1 = X1.permute(1, 0, 2)\n","        RNN_X, state = self.rnn(X1, state)\n","        RNN_X = RNN_X[-1]\n","        # print(RNN_X.size())\n","        output = self.fc1(RNN_X)\n","        # print(output.size())\n","        return output\n","\n","\n","if __name__ == '__main__':\n","    embedding_size = 2\n","    n_hidden = 5\n","\n","    file_list = ['I hate milk', 'Its cold today', 'I like cat']\n","    batch_size = len(file_list)\n","\n","    word_vocab = ' '.join(file_list).split()\n","    word_vocab = list(set(word_vocab))\n","    word2index = {w: i for i, w in enumerate(word_vocab)}\n","    index2word = {i: w for i, w in enumerate(word_vocab)}\n","\n","    n_seq = len(word2index)\n","\n","    input = [i.split(' ')[:-1] for i in file_list]\n","    map_input = [[word2index[i] for i in j] for j in input]\n","    target = [i.split(' ')[-1] for i in file_list]\n","    map_target = [word2index[i] for i in target]\n","\n","    model = TextRNN()\n","    lr = 1e-3\n","    epoch = 5000\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    inputs = torch.LongTensor(map_input)\n","    targets = torch.LongTensor(map_target)\n","\n","    # Training\n","    for ep in range(epoch):\n","        optimizer.zero_grad()\n","\n","        state = None\n","        output = model(state, inputs)\n","\n","        loss = criterion(output, targets)\n","        if (ep + 1) % 1000 == 0:\n","            print('Epoch:', '%04d' % (ep + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    input = [sentence.split()[:2] for sentence in file_list]\n","\n","    # Test\n","    state = None\n","    predict = model(state, inputs).data.max(1, keepdim=True)[1]\n","    print(\"Input:\", [sentence.split()[:2] for sentence in file_list])\n","    print(\"Output:\", [index2word[word.item()] for word in predict.squeeze()])"]}]}