{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPd65PewfwXauqIriXf9jbo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ZddViq_y6YmW","executionInfo":{"status":"ok","timestamp":1677753737002,"user_tz":360,"elapsed":36988,"user":{"displayName":"Jesse Liu","userId":"09638298407322875956"}},"outputId":"ce9a44ac-776e-48ee-c4c7-217385cd8b79","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1000 cost = 0.036333\n","Epoch: 2000 cost = 0.007272\n","Epoch: 3000 cost = 0.003088\n","Epoch: 4000 cost = 0.001579\n","Epoch: 5000 cost = 0.000867\n","Input: [['He', 'hate'], ['Its', 'cold'], ['I', 'like']]\n","Output: ['milk', 'today', 'cat']\n"]}],"source":["# jesseLiu2000\n","from unicodedata import bidirectional\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","class TextLSTM(nn.Module):\n","    def __init__(self):\n","        super(TextLSTM, self).__init__()\n","        self.embedding = nn.Embedding(n_seq, embedding_size)\n","        self.bilstm = nn.LSTM(input_size=embedding_size, hidden_size=n_hidden, bidirectional=True, num_layers=4)\n","        self.fc1 = nn.Linear(n_hidden*2, n_seq)\n","\n","    def forward(self, state, X):\n","        X1 = self.embedding(X)\n","        X1 = X1.float()\n","        X1 = X1.permute(1, 0, 2)\n","        LSTM_X, state = self.bilstm(X1, state)\n","        LSTM_X = LSTM_X[-1]  \n","        output = self.fc1(LSTM_X)\n","        return output\n","\n","\n","if __name__ == '__main__':\n","    embedding_size = 2\n","    n_hidden = 5\n","\n","    file_list = ['He hate milk', 'Its cold today', 'I like cat']\n","    batch_size = len(file_list)\n","\n","    word_vocab = ' '.join(file_list).split()\n","    word_vocab = list(set(word_vocab))\n","    word2index = {w: i for i, w in enumerate(word_vocab)}\n","    index2word = {i: w for i, w in enumerate(word_vocab)}\n","\n","    n_seq = len(word2index)\n","\n","    input = [i.split(' ')[:-1] for i in file_list]\n","    map_input = [[word2index[i] for i in j] for j in input]\n","    target = [i.split(' ')[-1] for i in file_list]\n","    map_target = [word2index[i] for i in target]\n","\n","    model = TextLSTM()\n","    lr = 1e-3\n","    epoch = 5000\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    inputs = torch.LongTensor(map_input)\n","    targets = torch.LongTensor(map_target)\n","\n","    # Training\n","    for ep in range(epoch):\n","        optimizer.zero_grad()\n","\n","        state = None\n","        output = model(state, inputs)\n","\n","        loss = criterion(output, targets)\n","        if (ep + 1) % 1000 == 0:\n","            print('Epoch:', '%04d' % (ep + 1), 'cost =', '{:.6f}'.format(loss))\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    input = [sentence.split()[:2] for sentence in file_list]\n","\n","    # Test\n","    state = None\n","    predict = model(state, inputs).data.max(1, keepdim=True)[1]\n","    print(\"Input:\", [sentence.split()[:2] for sentence in file_list])\n","    print(\"Output:\", [index2word[word.item()] for word in predict.squeeze()])"]}]}